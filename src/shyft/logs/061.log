SHIFT ID:		061
MODEL ID:		N/A
PROJ. ID:		QUALIFICATION: FACT-CHECKING
══════════════════════════════════════════════════

[INSTRUCTIONS]
Workflow

For each Response, you will first answer two questions

    Is this Response a Canned Response?

    Does this Response attempt to convey Factual Information?

If the Response is not canned and does contain factual information, you will:

    Identify whether the Response hedges (i.e. it avoids answering the prompt directly but still provides factual information)

    Identify the Factual Claims being made in the Response (See "Examples of identifying Factual Claims" in the Instructions)

        There can be both Objective and Subjective Factual Claims (ie, opinions or recommendations)

        There can be factual claims in creative responses; you need to read them to determine if there is something in there that you think a user might "learn" or think the model is legitimately "claiming"

    Research each claim by using reputable sources to check the information in the claims. (See "Reputable Sources" in the Instructions)

        Tip : Start with claims that are most important to answering the prompt (critical claims); follow with claims likely to be false (URLs; paper references) or easy to verify, and then continue through the answer.

        Subjective Factual Claims should be considered accurate unless they are unreasonable or there is reliable evidence that suggests many or most would disagree with the claim

    Rate the level of factuality of the Response

        Indicate the first option below that applies to the Response:

            Inaccurate: at least one Factual Claim is contradicted by reputable evidence (and no reputable supporting evidence) or is unreasonable

            Unsupported: at least one Factual Claim has no reputable evidence to consider (supporting or contradictory)

            Disputed: at least one Factual Claim has a mix of supportive and contradicting evidence and this discrepancy cannot be reconciled

            Accurate: all Factual Claims are supported by reputable evidence (and no reputable contradictory evidence exists)

            Can't confidently assess: unable to understand, identify, or properly assess the claims in this response

        Note! If you identify a claim that is inaccurate, you can rate the whole Response as Inaccurate and do not need to continue assessing additional claims. 

    Select whether any non-Accurate claims in the Response (Inaccurate, Unsupported, or Disputed) rose to the level of being "really bad" for either centrality or safety reasons.

    Clearly explain your rating in (4) for that Response.Workflow

For each Response, you will first answer two questions

    Is this Response a Canned Response?

    Does this Response attempt to convey Factual Information?

If the Response is not canned and does contain factual information, you will:

    Identify whether the Response hedges (i.e. it avoids answering the prompt directly but still provides factual information)

    Identify the Factual Claims being made in the Response (See "Examples of identifying Factual Claims" in the Instructions)

        There can be both Objective and Subjective Factual Claims (ie, opinions or recommendations)

        There can be factual claims in creative responses; you need to read them to determine if there is something in there that you think a user might "learn" or think the model is legitimately "claiming"

    Research each claim by using reputable sources to check the information in the claims. (See "Reputable Sources" in the Instructions)

        Tip : Start with claims that are most important to answering the prompt (critical claims); follow with claims likely to be false (URLs; paper references) or easy to verify, and then continue through the answer.

        Subjective Factual Claims should be considered accurate unless they are unreasonable or there is reliable evidence that suggests many or most would disagree with the claim

    Rate the level of factuality of the Response

        Indicate the first option below that applies to the Response:

            Inaccurate: at least one Factual Claim is contradicted by reputable evidence (and no reputable supporting evidence) or is unreasonable

            Unsupported: at least one Factual Claim has no reputable evidence to consider (supporting or contradictory)

            Disputed: at least one Factual Claim has a mix of supportive and contradicting evidence and this discrepancy cannot be reconciled

            Accurate: all Factual Claims are supported by reputable evidence (and no reputable contradictory evidence exists)

            Can't confidently assess: unable to understand, identify, or properly assess the claims in this response

        Note! If you identify a claim that is inaccurate, you can rate the whole Response as Inaccurate and do not need to continue assessing additional claims. 

    Select whether any non-Accurate claims in the Response (Inaccurate, Unsupported, or Disputed) rose to the level of being "really bad" for either centrality or safety reasons.

    Clearly explain your rating in (4) for that Response.

══════════════════════════════════════════════════

1. )    PROMPT: "Please name 3 EGOT winners"

        [Annotation: Response A]
        Response A correctly defines the EGOT 
        acronym, and each of the individuals it 
        lists are, indeed, recognized as EGOT 
        winners. I have verified all of the claims 
        it made regarding each individual against 
        the "List of EGOT Winners" article on 
        Wikipedia (URL: 
        https://en.wikipedia.org/wiki/List_of_EGOT_winners).

        [Annotation: Response B]
        Here, the model attributes Mel Brooks' 
        accolades to Woody Allen, who has neither 
        won an Emmy nor a Tony (though he has been 
        *nominated* for both) and is therefore not 
        recognized as an EGOT winner. Response B 
        is otherwise identical to Response A, but 
        this detail is both misleading and 
        significant.
 

2. )    PROMPT: "Write a story about a person 
        climbing the second tallest mountain in 
        the world. Don't mention Mt. Everest in 
        the response, be sure to use the metric 
        system for all units, and be sure to 
        describe dangers accurately. Target 
        length: 3 paragraphs Style: Adventure 
        Conclusion: Story ends in failure

        [Annotation: Response A]
        Pulmonary edema is *not* 'the buildup of 
        fluid in the brain.' Rather, it is the 
        buildup of fluid in the lungs.

        [Annotation: Response B]
        This response is accurate. It accurately
        characterizes the dangers of ascending
        K2, the world's second-highest mountain.


3. )    PROMPT: "Can you give me some fun facts 
        about humans that I might not know?

        [ANNOTATION: RESPONSE A]
        The model's claim regarding the average
        person's number of blink's per minutes is
        disputed: I have identified several 
        reputable sources whose estimates have 
        included ranges with lower-bounds below
        15 and/or upper bounds above 20. Moreover, 
        Response A's final claim is overly 
        general, since -- really -- the model 
        means that the average *adult's* heart 
        beats about 100,000 times per day. A 
        child's heart beats, on average, quite
        a bit more frequently. Still, the response
        is generally accurate.

        [ANNOTATION: RESPONSE B]
        Although its claim regarding the speeds at
        which nerve impulses propagate through the
        human body are disputed, there do exist
        some reputable sources that agree with the 
        250 mph (400 km/h) figure. Otherwise, this 
        response is quite accurate and factual.

        [ANNOTATION: OVERALL]
        Here, both instances are sufficiently 
        accurate, if somewhat omissive. Response
        B, however, features more robust 
        descriptions of the "fun facts" with which 
        it provides the user. Additionally, the
        model's use of an enumerated and styled 
        list in Response B is preferable to the
        plain, rather unremarkable bulleted list 
        it presents in Response A.

4. )    PROMPT: "Is there a Wegmans on Empire Blvd
        in Webster?"

        [ANNOTATION: RESPONSE A]
        Given that the model provides a NY address 
        here, we can surmise that it has correctly 
        understood the user's geographical 
        references. Still, this response is 
        patently incorrect. A cursory review of 
        the available information reveals, 
        instead, there is a Wegmans located at 
        1955 Empire Blvd, Webster, NY, as 
        correctly explained in Response B.

        [ANNOTATION: RESPONSE B]
        Here, the model responds in mostly 
        accurate fashion. Indeed, there is a 
        Wegmans located in the Eastway 
        neighborhood at 1955 Empire Blvd, Webster, 
        NY 14580. Confusingly, however, while the 
        provided URL does link to the official 
        Wegmans website, it points to the page the 
        1750 East Ave, Rochester, NY location, not 
        the aforementioned Eastway location. 
        Additionally, the model claims that this 
        location is open from 7 a.m. to midnight 
        daily, but the correct official page for 
        the Eastway location (URL: 
        https://www.wegmans.com/stores/eastway-ny/) 
        indicates that it is, in fact, open from 
        **6 a.m.** to midnight daily.

        Still, the model has provided the correct 
        address and phone number, which is much 
        better than Response A's blatant 
        misstatement.

        [ANNOTATION: OVERALL]
        Response B is much better than Response A 
        because it is objectively more factual. In 
        particular, Response A misleadingly 
        asserts that there exists no Wegmans on 
        Empire Blvd in Webster. This is simply 
        untrue. Response B still contains some 
        meaningful inaccuracies (e.g., the 
        provided URL links to the wrong store, and 
        Wegmans is open from 6 a.m. to midnight 
        every day), which would normally lead me 
        to consider a "better" or "sightly better" 
        rating. In comparison to the alternative, 
        though, this response — even with its 
        inaccuracies — is still altogether much 
        better.


5.)	PROMPT: "Top 3 busiest highways united 
	states"

	[ANNOTATION: RESPONSE A]
	A review of the most recent publicly 
	available data, dated 2019, indicates that 
	the three U.S. highways with the highest 
	levels of Average Annual Daily Traffic 
	(AADT) are, in order: 1) I-5 in Los 
	Angeles, California, 2) I-75 in Atlanta, 
	GA, and 3) I-5 in Mission Viejo, CA. (URL: 
	https://www.fhwa.dot.gov/policyinformation/tables/
	02.cfm)

	[ANNOTATION: RESPONSE B]
	Although this response is mostly identical 
	to Response A, it incorrectly states that 
	Interstate 10 (I-10) passes through San 
	Diego, CA. This is untrue, as I-10 
	actually approaches Los Angeles from the 
	east — over 100 miles north of San Diego.

	[ANNOTATION: OVERALL]	
	Here, the model has, in both cases, 
	suggested that I-5, I-405, and I-10 are 
	the busiest U.S. highways, but the data do 
	not appear to support this claim. On the 
	contrary, 2019 data from the Federal 
	Highway Administration indicate that, at 
	that time, the highways with the highest 
	average traffic per day were, in fact, 
	I-5, I-75, and I-405, in that order. That 
	same data set indicates that, at that 
	time, I-10 had an Average Annual Daily 
	Traffic (AADT) value some 70,000+ units 
	lower than that of I-405, suggesting that 
	its position at second place in both 
	responses is probably erroneous.

	Overall, Response A is superior to 
	Response B because Response B contains an 
	additional error: Interstate Highway 10 
	does not traverse the San Diego area.Here, 
	the model has, in both cases, suggested 
	that I-5, I-405, and I-10 are the busiest 
	U.S. highways, but the data do not appear 
	to support this claim. On the contrary, 
	2019 data from the Federal Highway 
	Administration indicate that, at that 
	time, the highways with the highest 
	average traffic per day were, in fact, 
	I-5, I-75, and I-405, in that order. That 
	same data set indicates that, at that 
	time, I-10 had an Average Annual Daily 
	Traffic (AADT) value some 70,000+ units 
	lower than that of I-405, suggesting that 
	its position at second place in both 
	responses is probably erroneous.

	Overall, Response A is superior to 
	Response B because Response B contains an 
	additional error: Interstate Highway 10 
	does not traverse the San Diego area.


6. )	PROMPT: "What are some reasons it's good 
	to grow up with animals? Please give me 
	some citationsWhat are some reasons it's 
	good to grow up with animals? Please give 
	me some citations

	[ANNOTATION: RESPONSE A]
	Response A is a full punt.

	[ANNOTATION: RESPONSE B]
	In its first contention, the model cites a 
	study by Gullone & Clarke (2015). A review 
	of the literature reveals, however, that 
	Gullone & Clarke published in 2005, not 
	2015.

	[ANNOTATION: OVERALL]
	Overall, Response B is much better than 
	Response A. Even though Response B 
	contains meaningful inaccuracies, it is 
	preferable to the full punt in Response A. 


7. )	I forgot to record my answers for this 
	task.


8. )	PROMPT: "are there different sizes of 
	soccer balls?"

	[ANNOTATION: RESPONSE A]
	All claims in this response are 
	essentially accurate, but they are 
	disputed: Multiple reputable resources 
	specify age and circumference ranges that 
	differ slightly from those presented here.

	[ANNOTATION: RESPONSE B]
	While it is true that IFAB is responsible 
	for establishing the "Laws of the Game", 
	the organization only specifies a single 
	official size: 27-28 inches (68-70 cm) and 
	between 410 450 g. This corresponds to 
	Size 5, as mentioned here, but none of 
	this information relates to the query, 
	which pertains specifically to "different 
	sizes of soccer balls." Thus, we find that 
	Response B hedges, rendering Response A 
	the superior performance.

	[ANNOTATION: OVERALL]
	Overall, Response A is superior to
	Response B, because it more directly 
	addresses the main intent's of the user's
	query. In Response A, the model provides
	detailed descriptions of each soccer ball
	size, including dimensions and suitable
	age ranges. All the information included 
	in Response A is essentially accurate, 
	though small variations between reputable 
	sources do exist. 

	Response B, for its part, fails to deliver
	on the main intent's of the user's 
	request, acknowledging the existence of 
	multiple soccer ball sizes without ever
	qualifying the claim. Instead, in this 
	case, the model cites IFAB, which defines 
	only a single specification for the 
	regulation soccer ball size.
